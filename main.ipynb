{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "The dataset contains text of online travel reviews (in Column Review) with an associated Rating (column Overall_Rating). \n",
    "The objective is to train a classifier to predict the rating from the Review text. \n",
    "You are free to choose the model's architecture, but you should describe and justify your design choices.\n",
    "Train the model and assess it as appropriate in machine learning. You are allowed to preprocess the data however you want \n",
    "(e.g. using pretrained embeddings, dropping some features, just a bag-of-words), but the predictive model must be trained \n",
    "by yourself from scratch (no pretrained predictor). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem definition\n",
    "- ***Dataset***: Online travel reviews with their corresponding ratings\n",
    "- ***Inputs***: Travel reviews\n",
    "- ***Output***: Predict the rating from the review\n",
    "\n",
    "We are training a classifier to predict the rating from the review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step.\n",
    "- **Load data**\n",
    "    - *loading the data*\n",
    "    - *splitting the data into train and test*\n",
    "- **Data preprocessing**\n",
    "    - **Text cleaning:** *Remove noise(pactuation, stopwords)*\n",
    "    - **Text normalization:** *Lowercasing, Stemming, Lemmantization*\n",
    "    - **Tokenization:** *Split text into words, or subwords, or characters*\n",
    "- **Feature extraction/embeddings**\n",
    "    - *Bag of words*\n",
    "    - *TF-IDF*\n",
    "- **Model Selection**\n",
    "    - *Naive Bayes classifier*\n",
    "- **Model Training**\n",
    "    - *Train on the tain dataset*\n",
    "    - *Monitor loss and accuracy on validation dataset*\n",
    "    - **Techniques**\n",
    "        - *Hyperparameter tuning*\n",
    "        - *Cross validation*\n",
    "        - *Regularization(dropout, weight decay)*\n",
    "    - *Train on the tain dataset*\n",
    "- **Model evaluation**\n",
    "    - **Metrics**\n",
    "        - *Accuracy, precision, recall, f1 score*\n",
    "        - *Confusion matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\school stuff\\master's - computer science - ai\\sem 2\\ispr\\assignments\\assignment 3\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.57.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\school stuff\\master's - computer science - ai\\sem 2\\ispr\\assignments\\assignment 3\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8\n",
      "  Using cached pillow-11.2.1-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\school stuff\\master's - computer science - ai\\sem 2\\ispr\\assignments\\assignment 3\\venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\school stuff\\master's - computer science - ai\\sem 2\\ispr\\assignments\\assignment 3\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in d:\\school stuff\\master's - computer science - ai\\sem 2\\ispr\\assignments\\assignment 3\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: numpy, tzdata, pytz, pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, tqdm, threadpoolctl, scipy, regex, pandas, matplotlib, joblib, click, seaborn, scikit-learn, nltk\n",
      "Successfully installed click-8.1.8 contourpy-1.3.0 cycler-0.12.1 fonttools-4.57.0 importlib-resources-6.5.2 joblib-1.5.0 kiwisolver-1.4.7 matplotlib-3.9.4 nltk-3.9.1 numpy-2.0.2 pandas-2.2.3 pillow-11.2.1 pyparsing-3.2.3 pytz-2025.2 regex-2024.11.6 scikit-learn-1.6.1 scipy-1.13.1 seaborn-0.13.2 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'd:\\SCHOOL STUFF\\MASTER'S - COMPUTER SCIENCE - AI\\SEM 2\\ISPR\\assignments\\assignment 3\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "## Installing some libraries and packages.\n",
    "%pip install pandas numpy matplotlib seaborn nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the necessary columns to read\n",
    "columns_to_read = [\"Overall_Rating\", \"Review\"]\n",
    "df = pd.read_csv(\"Airline_Reviews.csv\", usecols=columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overall_Rating    0\n",
       "Review            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if there is null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into dependent and independent variables \n",
    "X = df[\"Review\"]\n",
    "y = df[\"Overall_Rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
